{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet # pos\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import _stop_words as stop_words\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=PendingDeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand-labeled dataset. \n",
    "# Cleaned (not duplicates). \n",
    "# Resized to have the same size for all three classes\n",
    "data = []\n",
    "data_labels = []\n",
    "with open(\"Data/neg_u_13k.csv\", encoding=\"utf8\") as f:\n",
    "  for i in f: \n",
    "    #data.append(eval(i).decode())  # we don eval/decode because each line is a string of a binary string\n",
    "    #data.append(i.replace(\"b'\", '').replace('b\"', '').replace('\"\\n', '').replace(\"'\\n\", ''))\n",
    "    data.append(i)\n",
    "    data_labels.append('neg')\n",
    "with open(\"Data/pos_u_13k.csv\", encoding=\"utf8\") as f:\n",
    "  for i in f: \n",
    "    data.append(i)\n",
    "    data_labels.append('pos')\n",
    "with open(\"Data/neu_u_13k.csv\", encoding=\"utf8\") as f:\n",
    "  for i in f: \n",
    "    data.append(i)\n",
    "    data_labels.append('neu')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "df = pd.DataFrame(zip(data, data_labels), columns=['tweet', 'sentiment'])\n",
    "non_test, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(non_test, test_size=0.25, random_state=42)\n",
    "\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'- THEN let it fuck u in the ass'\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'!!!!!!!!!!!!!!!!!!!! @ms_fabdee: For the love of God, please smell nice.\"\"'\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'\" 6 children, 2 adults dead in Florida shooting: BELL, Fla. (AP) \\xe2\\x80\\x94 A once-convicted felon killed six ... http://t.co/BZw4fdoaZz #science'\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'\" Hopes for the future..? Yes. However , getting there with you...is not much different than being alone..!\"'\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'\" If you don\\'t have anything nice to say, don\\'t say anything at all.\"'\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b\"- isn't there, or that his beard looks weird, or that this whole fucking situation is completely odd!\"\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b\"- Like don't Nobody Want Me Frfr ,\"\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b'\" Love,you hurt my heart \"'\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b'\" Mfs stay feeling some type about shit I post on MY twitter ..Like if it bothers you so bad pray to Jesus about it maybe he\\'ll fix it \\xf0\\x9f\\x98\\xb9\\xf0\\x9f\\x91\\x8c\"'\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b'\" Mr Cereal lover, I wish your mother loved you like I would of that way you could of known how to love a woman \" \\xf0\\x9f\\x91\\x8f\\xf0\\x9f\\x99\\x8c'\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                            tweet  \\\n",
       "0  b'- THEN let it fuck u in the ass'\\n                                                                                                                                             \n",
       "1  b'!!!!!!!!!!!!!!!!!!!! @ms_fabdee: For the love of God, please smell nice.\"\"'\\n                                                                                                  \n",
       "2  b'\" 6 children, 2 adults dead in Florida shooting: BELL, Fla. (AP) \\xe2\\x80\\x94 A once-convicted felon killed six ... http://t.co/BZw4fdoaZz #science'\\n                         \n",
       "3  b'\" Hopes for the future..? Yes. However , getting there with you...is not much different than being alone..!\"'\\n                                                                \n",
       "4  b'\" If you don\\'t have anything nice to say, don\\'t say anything at all.\"'\\n                                                                                                     \n",
       "5  b\"- isn't there, or that his beard looks weird, or that this whole fucking situation is completely odd!\"\\n                                                                       \n",
       "6  b\"- Like don't Nobody Want Me Frfr ,\"\\n                                                                                                                                          \n",
       "7  b'\" Love,you hurt my heart \"'\\n                                                                                                                                                  \n",
       "8  b'\" Mfs stay feeling some type about shit I post on MY twitter ..Like if it bothers you so bad pray to Jesus about it maybe he\\'ll fix it \\xf0\\x9f\\x98\\xb9\\xf0\\x9f\\x91\\x8c\"'\\n   \n",
       "9  b'\" Mr Cereal lover, I wish your mother loved you like I would of that way you could of known how to love a woman \" \\xf0\\x9f\\x91\\x8f\\xf0\\x9f\\x99\\x8c'\\n                          \n",
       "\n",
       "  sentiment  \n",
       "0  neg       \n",
       "1  neg       \n",
       "2  neg       \n",
       "3  neg       \n",
       "4  neg       \n",
       "5  neg       \n",
       "6  neg       \n",
       "7  neg       \n",
       "8  neg       \n",
       "9  neg       "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Selection Model (33%)\n",
    "A random classification model would have around 33% accuracy as there are three classes and they are balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Rule-based (38%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.61\n"
     ]
    }
   ],
   "source": [
    "rule_pos = set()\n",
    "rule_neg = set()\n",
    "with open(\"Data/rule_neg.txt\", encoding=\"utf8\") as f:\n",
    "  for i in f: \n",
    "    rule_neg.add(i.rstrip('\\n').lower())\n",
    "with open(\"Data/rule_pos.txt\", encoding=\"utf8\") as f:\n",
    "  for i in f: \n",
    "    rule_pos.add(i.rstrip('\\n').lower())\n",
    "\n",
    "def label(string):\n",
    "  pos_count, neg_count = 0, 0\n",
    "  for i in string.split():\n",
    "    if i.lower() in rule_pos:\n",
    "      pos_count += 1\n",
    "    if i.lower() in rule_neg:\n",
    "      neg_count += 1\n",
    "  if pos_count == neg_count:\n",
    "    if pos_count == 0:\n",
    "      return 'neu'\n",
    "    else: \n",
    "      return 'neg'\n",
    "  if pos_count > neg_count:\n",
    "    return 'pos'\n",
    "  return 'neg'\n",
    "\n",
    "correct_count = 0\n",
    "for index, (tweet, sentiment) in df.iterrows():\n",
    "  if label(tweet) == sentiment:\n",
    "    correct_count += 1\n",
    "    \n",
    "print(round( (correct_count/len(df) * 100), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLTK API (48%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.9\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "\n",
    "import requests\n",
    "def nltk_label(text):\n",
    "  res = requests.post('http://text-processing.com/api/sentiment/', data={'text':text})\n",
    "  sentiment = eval(res.text)['label']\n",
    "  return sentiment if sentiment != 'neutral' else 'neu'\n",
    "\n",
    "correct_count = 0\n",
    "for index, (tweet, sentiment) in test.iterrows():\n",
    "  if index >= n:  # because of throttle\n",
    "    break\n",
    "  try:\n",
    "    if nltk_label(tweet) == sentiment:\n",
    "      correct_count += 1\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    print(tweet)\n",
    "    print(round( (correct_count/index * 100), 2))\n",
    "    break\n",
    "  \n",
    "    \n",
    "print(round( (correct_count/n * 100), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial ML Model without any improvements (59.82%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data, data_labels, vectorizer_params={}, model_params={}, model=None, verbose=False):  # for Naive Bayes: model = MultinomialNB()\n",
    "  \n",
    "  vectorizer = CountVectorizer(**vectorizer_params)\n",
    "  features = vectorizer.fit_transform(data)\n",
    "  feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "  X_train, X_val, y_train, y_val  = train_test_split(features, data_labels, test_size=0.2, random_state=42)\n",
    "  \n",
    "  if not model:\n",
    "    model = LogisticRegression(**model_params, solver='newton-cg')\n",
    "  \n",
    "  if verbose:\n",
    "    #print(vectorizer)\n",
    "    print('Number of Features: ', len(feature_names))\n",
    "    #print('model: ', model)\n",
    "    \n",
    "  model.fit(X=X_train, y=y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "  if verbose:\n",
    "    print('Accuracy: {}%'.format(round(accuracy_score(y_val, y_pred) * 100, 2)))\n",
    "  return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features:  47531\n",
      "Accuracy: 59.82%\n"
     ]
    }
   ],
   "source": [
    "feature_names = run_model(train['tweet'], train['sentiment'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283738"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = get_tokens(train['tweet'])\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Cleanup (60.39%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features:  27472\n",
      "Accuracy: 60.39%\n"
     ]
    }
   ],
   "source": [
    "def cleanup(text):\n",
    "  if text[:2] in ('b\"', \"b'\"):\n",
    "    text = text[2:]\n",
    "  # text = text.replace('_', ' ')\n",
    "  # remove_digits = str.maketrans('', '', '012456789')\n",
    "  # text = text.translate(remove_digits)\n",
    "\n",
    "  tokens = text.split()\n",
    "  filtered_tokens = ' '.join([ i for i in tokens if not i.startswith('http') and not i.startswith('@') ])\n",
    "  return filtered_tokens\n",
    "\n",
    "train_cleaned = train.copy()\n",
    "train_cleaned['tweet'] = train['tweet'].apply(cleanup)\n",
    "feature_names2 = run_model(train_cleaned['tweet'], train_cleaned['sentiment'], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding StopWords (didn't improve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** sklearn stopwords\n",
      "Number of Features:  27179\n",
      "Accuracy: 59.31%\n",
      "*** sklearn stopwords cleaned\n",
      "Number of Features:  27211\n",
      "Accuracy: 59.89%\n",
      "*** nltk stopwords\n",
      "Number of Features:  27331\n",
      "Accuracy: 59.8%\n",
      "*** nltk stopwords cleaned\n",
      "Number of Features:  27361\n",
      "Accuracy: 60.27%\n",
      "*** custome stopwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vvaezian\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['amp', 're', 'rt', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features:  27398\n",
      "Accuracy: 59.93%\n"
     ]
    }
   ],
   "source": [
    "# sklearn stopwords (default)\n",
    "print('*** sklearn stopwords')\n",
    "feature_names = run_model(train_cleaned['tweet'], train_cleaned['sentiment'], vectorizer_params={'stop_words':'english'}, verbose=True)\n",
    "\n",
    "# sklearn stopwords (default) cleaned\n",
    "from sklearn.feature_extraction import _stop_words as stop_words\n",
    "scikit_stopwords = stop_words.ENGLISH_STOP_WORDS\n",
    "scikit_stopwords_cleaned = set(scikit_stopwords)\n",
    "# removing the following words from the list of stopwords as they could be helpful in determining sentiment\n",
    "for member in ['empty', 'less', 'too', 'alone', 'never', 'enough', 'can', 'everything', 'give', 'serious', \n",
    "               'will', 'always', 'couldnt', 'nobody', 'must', 'sincere', 'cant', 'down', 'cannot', 'cry', \n",
    "               'full', 'neither', 'nowhere', 'anything', 'nor', 'nothing', 'not', 'please', \n",
    "               'last', 'behind', 'out', 'every']:\n",
    "  scikit_stopwords_cleaned.remove(member)\n",
    "print('*** sklearn stopwords cleaned')\n",
    "feature_names = run_model(train_cleaned['tweet'], train_cleaned['sentiment'], vectorizer_params={'stop_words':scikit_stopwords_cleaned}, verbose=True)\n",
    "\n",
    "# nltk stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "print('*** nltk stopwords')\n",
    "feature_names = run_model(train_cleaned['tweet'], train_cleaned['sentiment'], vectorizer_params={'stop_words':nltk_stopwords}, verbose=True)\n",
    "\n",
    "# nltk stopwords cleaned\n",
    "nltk_stopwords_cleaned=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"to\", \"from\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"more\", \"most\", \"other\", \"some\", \"such\", \"nor\", \"only\", \"own\", \"same\", \"so\", \"than\", \"very\", \"s\", \"t\", \"just\", \"don\", \"should\", \"now\"]\n",
    "print('*** nltk stopwords cleaned')\n",
    "feature_names = run_model(train_cleaned['tweet'], train_cleaned['sentiment'], vectorizer_params={'stop_words':nltk_stopwords_cleaned}, verbose=True)\n",
    "\n",
    "# the following is taken from the 100 most common tokens in the data that seems to have no sentiment\n",
    "custom_stopwords = {'I', 'the', 'RT', 'to', 'a', 'you', 'and', 'in', 'is', 'of', 'my', 'for', 'me', 'on', 'it'\n",
    "  , 'that', \"I'm\", 'be', 'so', 'this', 'with', 'your', 'have', 'at', 'just', 'are', 'but', 'i' \n",
    "  , 'was', 'all', 'get', 'up', 'do', 'when', '&amp;', '-', 'from', 'if', 'know', 'we', 'about', 'what'\n",
    "  , 'The', \"it's\", 'as', 'they', 'one', 'by', 'no', 'see', 'go', 'You', 'how'\n",
    "  , 'or', 'an', 'got', 'who', 'he', 'more', 'day', 'make', \"you're\", 'My', 'A', 'has', 'really', 'now', 'some'\n",
    "  , \"It's\", 'back', 'would', 'going', 'his', 'been', 'new', 'only', 'her', 'even'\n",
    "  , \"I've\", 'them', 'than', 'our', \"that's\", 'This', 'had', 'there', 'then', 'say', 'come', 'their', 'am'\n",
    "}\n",
    "\n",
    "print('*** custome stopwords')\n",
    "feature_names = run_model(train_cleaned['tweet'], train_cleaned['sentiment'], vectorizer_params={'stop_words':custom_stopwords}, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming and Lemmatization (didn't improve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studies study studi\n",
      "studied studied studi\n",
      "studying studying studi\n",
      "loves love love\n",
      "loved loved love\n",
      "came came came\n",
      "test_this test_this test_thi\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "for word in ['studies', 'studied', 'studying', 'loves', 'loved', 'came', 'test_this']:\n",
    "  print(word, lemmatizer.lemmatize(word), stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features:  27960\n",
      "Accuracy: 59.99%\n"
     ]
    }
   ],
   "source": [
    "def cleanup2(text):\n",
    "  return text.replace('_', ' ')\n",
    "  \n",
    "train_cleaned_stemmed = train_cleaned.copy()\n",
    "#train_cleaned_stemmed = train_cleaned_stemmed.apply(cleanup2)\n",
    "train_cleaned_stemmed['tweet'] = train_cleaned['tweet'].apply(stemmer.stem)\n",
    "feature_names3 = run_model(train_cleaned_stemmed['tweet'], train_cleaned_stemmed['sentiment'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features:  27956\n",
      "Accuracy: 60.03%\n"
     ]
    }
   ],
   "source": [
    "train_cleaned_stemmed = train_cleaned.copy()\n",
    "train_cleaned_stemmed['tweet'] = train_cleaned['tweet'].apply(cleanup2).apply(stemmer.stem)\n",
    "feature_names3 = run_model(train_cleaned_stemmed['tweet'], train_cleaned_stemmed['sentiment'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24773</th>\n",
       "      <td>rt me in a relationship \\xf0\\x9f\\x98\\xa1\\xf0\\x9f\\x98\\x8d</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3831</th>\n",
       "      <td>that's ugly. \\xf0\\x9f\\x98\\xb4\"</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36171</th>\n",
       "      <td>smooth criminal? nvm...lol</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17012</th>\n",
       "      <td>can you please follow me i love you so much \\xf0\\x9f\\x8c\\xba\\xf0\\x9f\\x8c\\xba\\xf0\\x9f\\x8c\\xba\\xf0\\x9f\\x8c\\xba \\xe2\\x9d\\xa4\\xef\\xb8\\x8f\\xe2\\x9d\\xa4\\xef\\xb8\\x8f\\xe2\\x9d\\xa4\\xef\\xb8\\x8f\\xe2\\x9d\\xa4\\xef\\xb8\\x8f\\xf0\\x9f\\x92\\x80\\xf0\\x9f\\x92\\x80\\xf0\\x9f\\x92\\x80hhaa</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33383</th>\n",
       "      <td>cc just came up with name listen to your kid</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38999</th>\n",
       "      <td>gadget: after $2m in pre-orders, osmo starts shipping its hardware-based ipad game for kids, rolls out customizati...</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20504</th>\n",
       "      <td>blessed to see another day.</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22050</th>\n",
       "      <td>i liked a video how the amazing spider-man should have ended - bonus scen</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4307</th>\n",
       "      <td>. has been unbelievable 2nd half #efc</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23350</th>\n",
       "      <td>one person followed me in the last day thanks to app</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                   tweet  \\\n",
       "24773  rt me in a relationship \\xf0\\x9f\\x98\\xa1\\xf0\\x9f\\x98\\x8d                                                                                                                                                                                                            \n",
       "3831   that's ugly. \\xf0\\x9f\\x98\\xb4\"                                                                                                                                                                                                                                      \n",
       "36171  smooth criminal? nvm...lol                                                                                                                                                                                                                                          \n",
       "17012  can you please follow me i love you so much \\xf0\\x9f\\x8c\\xba\\xf0\\x9f\\x8c\\xba\\xf0\\x9f\\x8c\\xba\\xf0\\x9f\\x8c\\xba \\xe2\\x9d\\xa4\\xef\\xb8\\x8f\\xe2\\x9d\\xa4\\xef\\xb8\\x8f\\xe2\\x9d\\xa4\\xef\\xb8\\x8f\\xe2\\x9d\\xa4\\xef\\xb8\\x8f\\xf0\\x9f\\x92\\x80\\xf0\\x9f\\x92\\x80\\xf0\\x9f\\x92\\x80hhaa   \n",
       "33383  cc just came up with name listen to your kid                                                                                                                                                                                                                        \n",
       "38999  gadget: after $2m in pre-orders, osmo starts shipping its hardware-based ipad game for kids, rolls out customizati...                                                                                                                                               \n",
       "20504  blessed to see another day.                                                                                                                                                                                                                                         \n",
       "22050  i liked a video how the amazing spider-man should have ended - bonus scen                                                                                                                                                                                           \n",
       "4307   . has been unbelievable 2nd half #efc                                                                                                                                                                                                                               \n",
       "23350  one person followed me in the last day thanks to app                                                                                                                                                                                                                \n",
       "\n",
       "      sentiment  \n",
       "24773  pos       \n",
       "3831   neg       \n",
       "36171  neu       \n",
       "17012  pos       \n",
       "33383  neu       \n",
       "38999  neu       \n",
       "20504  pos       \n",
       "22050  pos       \n",
       "4307   neg       \n",
       "23350  pos       "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cleaned_stemmed[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handling Emojis\n",
    "https://unicode.org/Public/emoji/15.0/emoji-test.txt\n",
    "- We need to make every emoji a new token (e.g. 'testðŸ˜¡ðŸ˜' is counted as one token, but it should be three tokens)\n",
    "- If we categorize emojis into groups and use group name, that like would improve the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "this is a test.ðŸ˜¡ðŸ˜\n"
     ]
    }
   ],
   "source": [
    "a = b'this is a test.\\xf0\\x9f\\x98\\xa1\\xf0\\x9f\\x98\\x8d'\n",
    "print(len(a))\n",
    "print(a.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'test.ðŸ˜¡ðŸ˜']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.decode().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So\n",
      "So\n"
     ]
    }
   ],
   "source": [
    "t = b'\\xf0\\x9f\\x98\\xa1\\xf0\\x9f\\x98\\x8d'\n",
    "for i in t.decode():\n",
    "  print(unicodedata.category(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'- THEN let it fuck u in the ass'\\n                                                                                                                                          \n",
       "1    b'!!!!!!!!!!!!!!!!!!!! @ms_fabdee: For the love of God, please smell nice.\"\"'\\n                                                                                               \n",
       "2    b'\" 6 children, 2 adults dead in Florida shooting: BELL, Fla. (AP) \\xe2\\x80\\x94 A once-convicted felon killed six ... http://t.co/BZw4fdoaZz #science'\\n                      \n",
       "3    b'\" Hopes for the future..? Yes. However , getting there with you...is not much different than being alone..!\"'\\n                                                             \n",
       "4    b'\" If you don\\'t have anything nice to say, don\\'t say anything at all.\"'\\n                                                                                                  \n",
       "5    b\"- isn't there, or that his beard looks weird, or that this whole fucking situation is completely odd!\"\\n                                                                    \n",
       "6    b\"- Like don't Nobody Want Me Frfr ,\"\\n                                                                                                                                       \n",
       "7    b'\" Love,you hurt my heart \"'\\n                                                                                                                                               \n",
       "8    b'\" Mfs stay feeling some type about shit I post on MY twitter ..Like if it bothers you so bad pray to Jesus about it maybe he\\'ll fix it \\xf0\\x9f\\x98\\xb9\\xf0\\x9f\\x91\\x8c\"'\\n\n",
       "9    b'\" Mr Cereal lover, I wish your mother loved you like I would of that way you could of known how to love a woman \" \\xf0\\x9f\\x91\\x8f\\xf0\\x9f\\x99\\x8c'\\n                       \n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet'].iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8    \" Mfs stay feeling some type about shit I post on MY twitter ..Like if it bothers you so bad pray to Jesus about it maybe he'll fix it ðŸ˜¹ðŸ‘Œ\"\n",
       "9    \" Mr Cereal lover, I wish your mother loved you like I would of that way you could of known how to love a woman \" ðŸ‘ðŸ™Œ                      \n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df['tweet'].iloc[8:10].map(lambda x: eval(x).decode())\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = get_tokens(df2)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" Mfs stay feeling some type about shit I post on MY twitter ..Like if it bothers you so bad pray to Jesus about it maybe he\\'ll fix it ðŸ˜¹ðŸ‘Œ\" \" Mr Cereal lover, I wish your mother loved you like I would of that way you could of known how to love a woman \" ðŸ‘ðŸ™Œ'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\"', 3),\n",
       " ('I', 3),\n",
       " ('it', 3),\n",
       " ('you', 3),\n",
       " ('about', 2),\n",
       " ('to', 2),\n",
       " ('of', 2),\n",
       " ('Mfs', 1),\n",
       " ('stay', 1),\n",
       " ('feeling', 1),\n",
       " ('some', 1),\n",
       " ('type', 1),\n",
       " ('shit', 1),\n",
       " ('post', 1),\n",
       " ('on', 1),\n",
       " ('MY', 1),\n",
       " ('twitter', 1),\n",
       " ('..Like', 1),\n",
       " ('if', 1),\n",
       " ('bothers', 1),\n",
       " ('so', 1),\n",
       " ('bad', 1),\n",
       " ('pray', 1),\n",
       " ('Jesus', 1),\n",
       " ('maybe', 1),\n",
       " (\"he'll\", 1),\n",
       " ('fix', 1),\n",
       " ('ðŸ˜¹ðŸ‘Œ\"', 1),\n",
       " ('Mr', 1),\n",
       " ('Cereal', 1),\n",
       " ('lover,', 1),\n",
       " ('wish', 1),\n",
       " ('your', 1),\n",
       " ('mother', 1),\n",
       " ('loved', 1),\n",
       " ('like', 1),\n",
       " ('would', 1),\n",
       " ('that', 1),\n",
       " ('way', 1),\n",
       " ('could', 1),\n",
       " ('known', 1),\n",
       " ('how', 1),\n",
       " ('love', 1),\n",
       " ('a', 1),\n",
       " ('woman', 1),\n",
       " ('ðŸ‘ðŸ™Œ', 1)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tokens).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test.:cat_with_tears_of_joy::OK_hand:'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "emoji.demojize('test.ðŸ˜¹ðŸ‘Œ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_animate_emoji = convert_animated_emojis(data)\n",
    "data_no_emoji = convert_text_emoji(data_no_animate_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver = 'liblinear', max_iter= 1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.633"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model(data_no_emoji, data_labels, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Handling Emoticos\n",
    "- In countVectorizer -> token_pattern param, it says \"punctuation is completely ignored and always treated as a token separator\". The messes up with emoticons (e.g. \":-)\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "compresed_data = compress(data_no_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.634"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model(compresed_data, data_labels, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ðŸ˜‚'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_emoji('xf0x9fx98x82')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ðŸ˜»'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u\"\\U0001f63b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize and find best accuracy for multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = range(1000, 4001, 200)\n",
    "Y = []\n",
    "best_acc = 0\n",
    "for n in X:\n",
    "  data = list(set(most_common(pos_lem_stem + neg_lem_stem, n)))\n",
    "  acc = run_model(data)\n",
    "  if acc > best_acc:\n",
    "    best_acc = acc\n",
    "  Y.append(acc) \n",
    "\n",
    "print('Best Accuracy: ', best_acc)\n",
    "plt.plot(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematization and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posStr = pos.lower() \n",
    "negStr = neg.lower()\n",
    "\n",
    "posStr2 = posStr.replace(\"\\'\", '')\n",
    "negStr2 = negStr.replace(\"\\'\", '')\n",
    "\n",
    "posStr = pos.lower().replace(\"n\\'t\", ' not')\n",
    "negStr = neg.lower().replace(\"n\\'t\", ' not')\n",
    "\n",
    "pos_cleaned = re.findall(\"[a-z][a-z]+\", posStr2)\n",
    "neg_cleaned = re.findall(\"[a-z][a-z]+\", negStr2)\n",
    "\n",
    "pos_lem_stem = [ lemmatizer.lemmatize(i) for i in pos_cleaned ]\n",
    "neg_lem_stem = [ lemmatizer.lemmatize(i) for i in neg_cleaned ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Misclassified Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_samples_pos = X_val[(y_val != y_pred) & (y_pred == 'pos')]\n",
    "misclassified_samples_neg = X_val[(y_val != y_pred) & (y_pred == 'neg')]\n",
    "res = vectorizer.inverse_transform(misclassified_samples_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.635\n"
     ]
    }
   ],
   "source": [
    "#model = LogisticRegression(**model_params, solver = 'liblinear', max_iter= 1000, random_state=0)\n",
    "model = LogisticRegression(solver = 'liblinear', max_iter= 1000, random_state=0)\n",
    "\n",
    "# for i in range(1000, 10001, 500):\n",
    "#   vocab = [i[0] for i in Counter(tokens).most_common(i)]\n",
    "#   run_model(data2_string_per_line, {'vocabulary':vocab})  # best 73.1\n",
    "\n",
    "# black_list = ['xe2x80xa6','xe2x80x9c', 'xe2x80x9d', 'xefxb8x8f', 'xe2x80x99']\n",
    "\n",
    "# for i in range(1000, 7501, 100):\n",
    "#   vocab = [i[0] for i in Counter(tokens).most_common(i)]\n",
    "#   print(i, run_model(data2_string_per_line, {'vocabulary':vocab}, model=model))  # best 73.1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
