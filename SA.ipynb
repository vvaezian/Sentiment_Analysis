{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet # pos\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import _stop_words as stop_words\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=PendingDeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand-labeled dataset. \n",
    "# Cleaned (not duplicates). \n",
    "# Resized to have the same size for all three classes\n",
    "data = []\n",
    "data_labels = []\n",
    "with open(\"Data/neg_u_13k.csv\", encoding=\"utf8\") as f:\n",
    "  for i in f: \n",
    "    data.append(cleanup(eval(i).decode()))  # we don eval/decode because each line is a string of a binary string\n",
    "    data_labels.append('neg')\n",
    "with open(\"Data/pos_u_13k.csv\", encoding=\"utf8\") as f:\n",
    "  for i in f: \n",
    "    data.append(cleanup(eval(i).decode()))\n",
    "    data_labels.append('pos')\n",
    "with open(\"Data/neu_u_13k.csv\", encoding=\"utf8\") as f:\n",
    "  for i in f: \n",
    "    data.append(cleanup(eval(i).decode())) \n",
    "    data_labels.append('neu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Rule_based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.92\n"
     ]
    }
   ],
   "source": [
    "rule_pos = set()\n",
    "rule_neg = set()\n",
    "with open(\"Data/rule_neg.txt\", encoding=\"utf8\") as f:\n",
    "  for i in f: \n",
    "    rule_neg.add(i.rstrip('\\n').lower())\n",
    "with open(\"Data/rule_pos.txt\", encoding=\"utf8\") as f:\n",
    "  for i in f: \n",
    "    rule_pos.add(i.rstrip('\\n').lower())\n",
    "\n",
    "def label(string):\n",
    "  pos_count, neg_count = 0, 0\n",
    "  for i in string.split():\n",
    "    if i.lower() in rule_pos:\n",
    "      pos_count += 1\n",
    "    if i.lower() in rule_neg:\n",
    "      neg_count += 1\n",
    "  if pos_count == neg_count == 0:\n",
    "    return 'neu'\n",
    "  if pos_count > neg_count:\n",
    "    return 'pos'\n",
    "  return 'neg'\n",
    "\n",
    "correct_count = 0\n",
    "for index, tweet in enumerate(data):\n",
    "  if label(tweet) == data_labels[index]:\n",
    "    correct_count += 1\n",
    "    \n",
    "print(round( (correct_count/len(data) * 100), 2))\n",
    "\n",
    "#TODO: Apply stemming (lover, loving, loved, etc all should be turned into love)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, data_labels, vectorizer_params={}, model_params={}, model=None, verbose=False):  # for Naive Bayes: model = MultinomialNB()\n",
    "  \n",
    "  vectorizer = CountVectorizer(**vectorizer_params)\n",
    "  if verbose:\n",
    "    print(vectorizer)\n",
    "  \n",
    "  # warnings.filterwarnings(\"ignore\", category=PendingDeprecationWarning)\n",
    "  # warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "  # warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "  # warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "  features = vectorizer.fit_transform(data)\n",
    "  if verbose:\n",
    "    print('Number of Features: ', len(vectorizer.get_feature_names()))\n",
    "  \n",
    "  X_train, X_val, y_train, y_val  = train_test_split(features, data_labels, test_size=0.2, random_state=42)\n",
    "  \n",
    "  if not model:\n",
    "    model = LogisticRegression(**model_params, solver='liblinear')\n",
    "    \n",
    "  model.fit(X=X_train, y=y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  return round(accuracy_score(y_val, y_pred) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.21"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(data, data_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_animate_emoji = convert_animated_emojis(data)\n",
    "data_no_emoji = convert_text_emoji(data_no_animate_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver = 'liblinear', max_iter= 1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.633"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(data_no_emoji, data_labels, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "compresed_data = compress(data_no_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.634"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(compresed_data, data_labels, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.582\n",
      "0.627\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    "scikit_stopwords = stop_words.ENGLISH_STOP_WORDS\n",
    "\n",
    "print(train(data, vectorizer_params={'stop_words':scikit_stopwords}, model=model))\n",
    "print(train(data_no_emoji, vectorizer_params={'stop_words':scikit_stopwords}, model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <_io.FileIO name='C:\\\\Users\\\\vvaezian.NET\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\english' mode='rb' closefd=True>\n",
      "ResourceWarning: unclosed file <_io.BufferedReader name='C:\\\\Users\\\\vvaezian.NET\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\stopwords\\\\english'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584\n",
      "0.628\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "\n",
    "print(train(data, vectorizer_params={'stop_words':nltk_stopwords}, model=model))\n",
    "print(train(data_no_emoji, vectorizer_params={'stop_words':nltk_stopwords}, model=model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords_cleaned=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472997"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = get_tokens(data)\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 10212),\n",
       " ('to', 9284),\n",
       " ('you', 8270),\n",
       " ('and', 5463),\n",
       " ('brt', 4935),\n",
       " ('in', 4521),\n",
       " ('is', 4505),\n",
       " ('my', 4406),\n",
       " ('of', 4270),\n",
       " ('for', 4116),\n",
       " ('rt', 4041),\n",
       " ('me', 3638),\n",
       " ('it', 3461),\n",
       " ('on', 3089),\n",
       " ('that', 2942),\n",
       " ('im', 2850),\n",
       " ('so', 2774),\n",
       " ('this', 2648),\n",
       " ('xf0x9fx98x82', 2645),\n",
       " ('be', 2549),\n",
       " ('xe2x80xa6', 2211),\n",
       " ('with', 2162),\n",
       " ('your', 2099),\n",
       " ('like', 2053),\n",
       " ('have', 2023),\n",
       " ('just', 1887),\n",
       " ('but', 1855),\n",
       " ('at', 1853),\n",
       " ('love', 1840),\n",
       " ('its', 1823),\n",
       " ('not', 1772),\n",
       " ('are', 1768),\n",
       " ('dont', 1751),\n",
       " ('if', 1586),\n",
       " ('all', 1485),\n",
       " ('we', 1413),\n",
       " ('xe2x80x9c', 1412),\n",
       " ('when', 1394),\n",
       " ('get', 1388),\n",
       " ('xe2x80x9d', 1378),\n",
       " ('can', 1360),\n",
       " ('do', 1359),\n",
       " ('was', 1358),\n",
       " ('up', 1337),\n",
       " ('what', 1260),\n",
       " ('out', 1240),\n",
       " ('xefxb8x8f', 1218),\n",
       " ('bi', 1207),\n",
       " ('no', 1145),\n",
       " ('from', 1112),\n",
       " ('know', 1102),\n",
       " ('about', 1067),\n",
       " ('will', 1062),\n",
       " ('they', 1060),\n",
       " ('one', 1031),\n",
       " ('good', 1021),\n",
       " ('people', 1001),\n",
       " ('as', 984),\n",
       " ('follow', 951),\n",
       " ('how', 936),\n",
       " ('go', 932),\n",
       " ('he', 919),\n",
       " ('now', 892),\n",
       " ('day', 892),\n",
       " ('by', 889),\n",
       " ('cant', 883),\n",
       " ('see', 868),\n",
       " ('want', 841),\n",
       " ('more', 831),\n",
       " ('time', 829),\n",
       " ('xf0x9fx98xad', 828),\n",
       " ('who', 822),\n",
       " ('lol', 813),\n",
       " ('xf0x9fx98x8d', 793),\n",
       " ('please', 790),\n",
       " ('got', 774),\n",
       " ('or', 770),\n",
       " ('youre', 770),\n",
       " ('an', 758),\n",
       " ('much', 746),\n",
       " ('new', 745),\n",
       " ('make', 730),\n",
       " ('xe2x9dxa4', 725),\n",
       " ('need', 715),\n",
       " ('thats', 691),\n",
       " ('too', 680),\n",
       " ('why', 668),\n",
       " ('really', 664),\n",
       " ('back', 663),\n",
       " ('think', 653),\n",
       " ('some', 634),\n",
       " ('has', 631),\n",
       " ('still', 607),\n",
       " ('his', 603),\n",
       " ('xf0x9fx98xa9', 598),\n",
       " ('would', 596),\n",
       " ('never', 596),\n",
       " ('happy', 587),\n",
       " ('them', 581),\n",
       " ('shit', 578),\n",
       " ('going', 562),\n",
       " ('life', 562),\n",
       " ('only', 552),\n",
       " ('there', 548),\n",
       " ('today', 547),\n",
       " ('she', 545),\n",
       " ('her', 544),\n",
       " ('thanks', 541),\n",
       " ('been', 539),\n",
       " ('even', 535),\n",
       " ('our', 516),\n",
       " ('xe2x80x99', 504),\n",
       " ('thank', 501),\n",
       " ('ive', 498),\n",
       " ('here', 480),\n",
       " ('great', 478),\n",
       " ('were', 468),\n",
       " ('than', 467),\n",
       " ('had', 466),\n",
       " ('come', 464),\n",
       " ('then', 464),\n",
       " ('right', 462),\n",
       " ('best', 457),\n",
       " ('well', 454),\n",
       " ('fuck', 452),\n",
       " ('am', 442),\n",
       " ('over', 441),\n",
       " ('always', 440),\n",
       " ('us', 439),\n",
       " ('someone', 437),\n",
       " ('xf0x9fx92x95', 433),\n",
       " ('say', 425),\n",
       " ('ill', 425),\n",
       " ('ever', 425),\n",
       " ('last', 421),\n",
       " ('should', 421),\n",
       " ('xf0x9fx98x98', 421),\n",
       " ('because', 418),\n",
       " ('feel', 418),\n",
       " ('way', 409),\n",
       " ('off', 406),\n",
       " ('could', 404),\n",
       " ('man', 401),\n",
       " ('hate', 399),\n",
       " ('did', 392),\n",
       " ('night', 390),\n",
       " ('their', 382),\n",
       " ('miss', 380),\n",
       " ('xf0x9fx98x92', 378),\n",
       " ('take', 376),\n",
       " ('look', 374),\n",
       " ('him', 371),\n",
       " ('work', 368),\n",
       " ('birthday', 368),\n",
       " ('these', 355),\n",
       " ('being', 350),\n",
       " ('xf0x9fx98x8a', 349),\n",
       " ('gonna', 346),\n",
       " ('hope', 346),\n",
       " ('better', 344),\n",
       " ('first', 339),\n",
       " ('school', 339),\n",
       " ('after', 335),\n",
       " ('girl', 331),\n",
       " ('wanna', 328),\n",
       " ('something', 323),\n",
       " ('oh', 309),\n",
       " ('xf0x9fx91x8c', 308),\n",
       " ('via', 307),\n",
       " ('stop', 306),\n",
       " ('game', 305),\n",
       " ('give', 304),\n",
       " ('getting', 302),\n",
       " ('xe2x99xa5', 301),\n",
       " ('things', 300),\n",
       " ('real', 296),\n",
       " ('bad', 296),\n",
       " ('keep', 295),\n",
       " ('same', 295),\n",
       " ('xf0x9fx99x8c', 293),\n",
       " ('thing', 292),\n",
       " ('into', 290),\n",
       " ('wait', 288),\n",
       " ('down', 288),\n",
       " ('world', 285),\n",
       " ('bmy', 283),\n",
       " ('hes', 281),\n",
       " ('tell', 280),\n",
       " ('god', 280),\n",
       " ('next', 280),\n",
       " ('again', 279),\n",
       " ('didnt', 279),\n",
       " ('aint', 278),\n",
       " ('every', 277),\n",
       " ('big', 276),\n",
       " ('everyone', 275),\n",
       " ('fucking', 274),\n",
       " ('let', 274),\n",
       " ('guys', 273),\n",
       " ('where', 272)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tokens).most_common(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ðŸ˜‚'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_emoji('xf0x9fx98x82')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize and find best accuracy for multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = range(1000, 4001, 200)\n",
    "Y = []\n",
    "best_acc = 0\n",
    "for n in X:\n",
    "  data = list(set(most_common(pos_lem_stem + neg_lem_stem, n)))\n",
    "  acc = train(data)\n",
    "  if acc > best_acc:\n",
    "    best_acc = acc\n",
    "  Y.append(acc) \n",
    "\n",
    "print('Best Accuracy: ', best_acc)\n",
    "plt.plot(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "  def __init__(self):\n",
    "    self.wnl = WordNetLemmatizer()\n",
    "  def __call__(self, articles):\n",
    "    #a = [self.wnl.lemmatize(compress(t)) for t in word_tokenize(articles) if '/' not in t and len(t) >= 2 and t.isalpha()]\n",
    "    #a = [self.wnl.lemmatize(compress(t), map_pos(nltk.pos_tag(t)[0][1])) for t in word_tokenize(articles) if '/' not in t and t.isalpha()]\n",
    "    a = [self.wnl.lemmatize(t, map_pos(nltk.pos_tag(t)[0][1])) for t in word_tokenize(articles) if '/' not in t and t.isalpha()]\n",
    "    return [i for i in a if len(i) > 1]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posStr = pos.lower() \n",
    "negStr = neg.lower()\n",
    "\n",
    "posStr2 = posStr.replace(\"\\'\", '')\n",
    "negStr2 = negStr.replace(\"\\'\", '')\n",
    "\n",
    "posStr = pos.lower().replace(\"n\\'t\", ' not')\n",
    "negStr = neg.lower().replace(\"n\\'t\", ' not')\n",
    "\n",
    "pos_cleaned = re.findall(\"[a-z][a-z]+\", posStr2)\n",
    "neg_cleaned = re.findall(\"[a-z][a-z]+\", negStr2)\n",
    "\n",
    "pos_lem_stem = [ lemmatizer.lemmatize(i) for i in pos_cleaned ]\n",
    "neg_lem_stem = [ lemmatizer.lemmatize(i) for i in neg_cleaned ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Misclassified Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_samples_pos = X_val[(y_val != y_pred) & (y_pred == 'pos')]\n",
    "misclassified_samples_neg = X_val[(y_val != y_pred) & (y_pred == 'neg')]\n",
    "res = vectorizer.inverse_transform(misclassified_samples_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.635\n"
     ]
    }
   ],
   "source": [
    "#model = LogisticRegression(**model_params, solver = 'liblinear', max_iter= 1000, random_state=0)\n",
    "model = LogisticRegression(solver = 'liblinear', max_iter= 1000, random_state=0)\n",
    "\n",
    "# for i in range(1000, 10001, 500):\n",
    "#   vocab = [i[0] for i in Counter(tokens).most_common(i)]\n",
    "#   train(data2_string_per_line, {'vocabulary':vocab})  # best 73.1\n",
    "\n",
    "# black_list = ['xe2x80xa6','xe2x80x9c', 'xe2x80x9d', 'xefxb8x8f', 'xe2x80x99']\n",
    "\n",
    "# for i in range(1000, 7501, 100):\n",
    "#   vocab = [i[0] for i in Counter(tokens).most_common(i)]\n",
    "#   print(i, train(data2_string_per_line, {'vocabulary':vocab}, model=model))  # best 73.1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
