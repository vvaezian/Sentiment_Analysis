{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet # pos\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import _stop_words as stop_words\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=PendingDeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand-labeled dataset. \n",
    "# Cleaned (not duplicates). \n",
    "# Resized to have the same size for all three classes\n",
    "data = []\n",
    "data_labels = []\n",
    "with open(\"Data/neg_u_13k.csv\", encoding=\"utf8\") as f:\n",
    "  for i in f: \n",
    "    #data.append(eval(i).decode())  # we don eval/decode because each line is a string of a binary string\n",
    "    #data.append(i.replace(\"b'\", '').replace('b\"', '').replace('\"\\n', '').replace(\"'\\n\", ''))\n",
    "    data.append(i)\n",
    "    data_labels.append('neg')\n",
    "with open(\"Data/pos_u_13k.csv\", encoding=\"utf8\") as f:\n",
    "  for i in f: \n",
    "    data.append(i)\n",
    "    data_labels.append('pos')\n",
    "with open(\"Data/neu_u_13k.csv\", encoding=\"utf8\") as f:\n",
    "  for i in f: \n",
    "    data.append(i)\n",
    "    data_labels.append('neu')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "df = pd.DataFrame(zip(data, data_labels), columns=['tweet', 'sentiment'])\n",
    "non_test, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(non_test, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'- THEN let it fuck u in the ass'\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'!!!!!!!!!!!!!!!!!!!! @ms_fabdee: For the love of God, please smell nice.\"\"'\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'\" 6 children, 2 adults dead in Florida shooting: BELL, Fla. (AP) \\xe2\\x80\\x94 A once-convicted felon killed six ... http://t.co/BZw4fdoaZz #science'\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'\" Hopes for the future..? Yes. However , getting there with you...is not much different than being alone..!\"'\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'\" If you don\\'t have anything nice to say, don\\'t say anything at all.\"'\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b\"- isn't there, or that his beard looks weird, or that this whole fucking situation is completely odd!\"\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b\"- Like don't Nobody Want Me Frfr ,\"\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b'\" Love,you hurt my heart \"'\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b'\" Mfs stay feeling some type about shit I post on MY twitter ..Like if it bothers you so bad pray to Jesus about it maybe he\\'ll fix it \\xf0\\x9f\\x98\\xb9\\xf0\\x9f\\x91\\x8c\"'\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b'\" Mr Cereal lover, I wish your mother loved you like I would of that way you could of known how to love a woman \" \\xf0\\x9f\\x91\\x8f\\xf0\\x9f\\x99\\x8c'\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b\"- you're crazy and I'm out of my mind. http://t.co/t6gx6GoEYD\"\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>b\"#AhoraSuena Thelma Houston - Don't leave me this way #TodoMenosMiedo http://t.co/Z76IoL81Ac\"\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b\"#AskDems I'm still a bit hazy on this #WarNotWar thing. When will you be perfectly clear about this?\"\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>b\"#bugsongs  Too many butterflies in this world, there's too many dreams that are broken in two...\"\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b\"#DANIEL_KENU....You're a MEGA disgrace to yourself,The Graphic Group,the Media Fraternity &amp;amp; your generation as a whole.. #SHAME\"\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b\"#evequotes Junior year Eve doesn't sleep! WAKE UP EVE! WAKE UP WAKE UP WAKE UP! *slaps self in face* (yes, this actually happened)\"\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>b\"#Fisher: 'Miscommunication' led to Winston dressing out\"\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>b\"#forwardpass what surely not bath with a forward pass it's maybe there 15th of the game !!! The only player doing anything is Harrison !!!\"\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>b\"#HipsterComicBooks Ruin everyone's concert experience man @midnight\"\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>b\"#ICantFuckWithYouIf you don't buy me no food \\xf0\\x9f\\x98\\x82\\xf0\\x9f\\x99\\x85\"\\n</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                             tweet  \\\n",
       "0   b'- THEN let it fuck u in the ass'\\n                                                                                                                                             \n",
       "1   b'!!!!!!!!!!!!!!!!!!!! @ms_fabdee: For the love of God, please smell nice.\"\"'\\n                                                                                                  \n",
       "2   b'\" 6 children, 2 adults dead in Florida shooting: BELL, Fla. (AP) \\xe2\\x80\\x94 A once-convicted felon killed six ... http://t.co/BZw4fdoaZz #science'\\n                         \n",
       "3   b'\" Hopes for the future..? Yes. However , getting there with you...is not much different than being alone..!\"'\\n                                                                \n",
       "4   b'\" If you don\\'t have anything nice to say, don\\'t say anything at all.\"'\\n                                                                                                     \n",
       "5   b\"- isn't there, or that his beard looks weird, or that this whole fucking situation is completely odd!\"\\n                                                                       \n",
       "6   b\"- Like don't Nobody Want Me Frfr ,\"\\n                                                                                                                                          \n",
       "7   b'\" Love,you hurt my heart \"'\\n                                                                                                                                                  \n",
       "8   b'\" Mfs stay feeling some type about shit I post on MY twitter ..Like if it bothers you so bad pray to Jesus about it maybe he\\'ll fix it \\xf0\\x9f\\x98\\xb9\\xf0\\x9f\\x91\\x8c\"'\\n   \n",
       "9   b'\" Mr Cereal lover, I wish your mother loved you like I would of that way you could of known how to love a woman \" \\xf0\\x9f\\x91\\x8f\\xf0\\x9f\\x99\\x8c'\\n                          \n",
       "10  b\"- you're crazy and I'm out of my mind. http://t.co/t6gx6GoEYD\"\\n                                                                                                               \n",
       "11  b\"#AhoraSuena Thelma Houston - Don't leave me this way #TodoMenosMiedo http://t.co/Z76IoL81Ac\"\\n                                                                                 \n",
       "12  b\"#AskDems I'm still a bit hazy on this #WarNotWar thing. When will you be perfectly clear about this?\"\\n                                                                        \n",
       "13  b\"#bugsongs  Too many butterflies in this world, there's too many dreams that are broken in two...\"\\n                                                                            \n",
       "14  b\"#DANIEL_KENU....You're a MEGA disgrace to yourself,The Graphic Group,the Media Fraternity &amp; your generation as a whole.. #SHAME\"\\n                                         \n",
       "15  b\"#evequotes Junior year Eve doesn't sleep! WAKE UP EVE! WAKE UP WAKE UP WAKE UP! *slaps self in face* (yes, this actually happened)\"\\n                                          \n",
       "16  b\"#Fisher: 'Miscommunication' led to Winston dressing out\"\\n                                                                                                                     \n",
       "17  b\"#forwardpass what surely not bath with a forward pass it's maybe there 15th of the game !!! The only player doing anything is Harrison !!!\"\\n                                  \n",
       "18  b\"#HipsterComicBooks Ruin everyone's concert experience man @midnight\"\\n                                                                                                         \n",
       "19  b\"#ICantFuckWithYouIf you don't buy me no food \\xf0\\x9f\\x98\\x82\\xf0\\x9f\\x99\\x85\"\\n                                                                                               \n",
       "\n",
       "   sentiment  \n",
       "0   neg       \n",
       "1   neg       \n",
       "2   neg       \n",
       "3   neg       \n",
       "4   neg       \n",
       "5   neg       \n",
       "6   neg       \n",
       "7   neg       \n",
       "8   neg       \n",
       "9   neg       \n",
       "10  neg       \n",
       "11  neg       \n",
       "12  neg       \n",
       "13  neg       \n",
       "14  neg       \n",
       "15  neg       \n",
       "16  neg       \n",
       "17  neg       \n",
       "18  neg       \n",
       "19  neg       "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Selection Model\n",
    "A random classification model would have around 33% accuracy as there are three classes and they are balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NLTK API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def nltk_label(text):\n",
    "  res = requests.post('http://text-processing.com/api/sentiment/', data={'text':text})\n",
    "  sentiment = eval(res.text)['label']\n",
    "  return sentiment if sentiment != 'neutral' else 'neu'\n",
    "\n",
    "# correct_count = 0\n",
    "# for index, (tweet, sentiment) in test.iterrows():\n",
    "#   if index >= 1000:  # because of throttle\n",
    "#     break\n",
    "#   try:\n",
    "#     if nltk_label(tweet) == sentiment:\n",
    "#       correct_count += 1\n",
    "#   except Exception as e:\n",
    "#     print(e)\n",
    "#     print(tweet)\n",
    "#     break\n",
    "  \n",
    "    \n",
    "# print(round( (correct_count/1000 * 100), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Rule-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.61\n"
     ]
    }
   ],
   "source": [
    "rule_pos = set()\n",
    "rule_neg = set()\n",
    "with open(\"Data/rule_neg.txt\", encoding=\"utf8\") as f:\n",
    "  for i in f: \n",
    "    rule_neg.add(i.rstrip('\\n').lower())\n",
    "with open(\"Data/rule_pos.txt\", encoding=\"utf8\") as f:\n",
    "  for i in f: \n",
    "    rule_pos.add(i.rstrip('\\n').lower())\n",
    "\n",
    "def label(string):\n",
    "  pos_count, neg_count = 0, 0\n",
    "  for i in string.split():\n",
    "    if i.lower() in rule_pos:\n",
    "      pos_count += 1\n",
    "    if i.lower() in rule_neg:\n",
    "      neg_count += 1\n",
    "  if pos_count == neg_count:\n",
    "    if pos_count == 0:\n",
    "      return 'neu'\n",
    "    else: \n",
    "      return 'neg'\n",
    "  if pos_count > neg_count:\n",
    "    return 'pos'\n",
    "  return 'neg'\n",
    "\n",
    "correct_count = 0\n",
    "for index, (tweet, sentiment) in df.iterrows():\n",
    "  if label(tweet) == sentiment:\n",
    "    correct_count += 1\n",
    "    \n",
    "print(round( (correct_count/len(df) * 100), 2))\n",
    "\n",
    "#TODO: Apply stemming (lover, loving, loved, etc all should be turned into love)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data, data_labels, vectorizer_params={}, model_params={}, model=None, verbose=False):  # for Naive Bayes: model = MultinomialNB()\n",
    "  \n",
    "  vectorizer = CountVectorizer(**vectorizer_params)\n",
    "  features = vectorizer.fit_transform(data)\n",
    "  \n",
    "  X_train, X_val, y_train, y_val  = train_test_split(features, data_labels, test_size=0.2, random_state=42)\n",
    "  \n",
    "  if not model:\n",
    "    model = LogisticRegression(**model_params, solver='newton-cg')\n",
    "  \n",
    "  if verbose:\n",
    "    #print(vectorizer)\n",
    "    print('Number of Features: ', len(vectorizer.get_feature_names()))\n",
    "    #print('model: ', model)\n",
    "    \n",
    "  model.fit(X=X_train, y=y_train)\n",
    "  y_pred = model.predict(X_val)\n",
    "\n",
    "  return round(accuracy_score(y_val, y_pred) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features:  47531\n",
      "Accuracy: 59.82%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {}%'.format(run_model(train['tweet'], train['sentiment'], verbose=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features:  42281\n",
      "Accuracy: 60.01%\n"
     ]
    }
   ],
   "source": [
    "def cleanup(text):\n",
    "  '''Remove non-informative words and characters (punctuation, extra space, ...)'''\n",
    "  tokens = text.split()\n",
    "  filtered_tokens = ' '.join([ i for i in tokens if not i.startswith('http') \n",
    "                                                #and not i.startswith('@') \n",
    "                                                #and len(i) > 1 \n",
    "                            ])\n",
    "  return filtered_tokens\n",
    "\n",
    "train2 = train.copy()\n",
    "train2['tweet'] = train['tweet'].apply(cleanup)\n",
    "print('Accuracy: {}%'.format(run_model(train2['tweet'], train2['sentiment'], verbose=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features:  33821\n",
      "Accuracy: 60.37%\n"
     ]
    }
   ],
   "source": [
    "def cleanup(text):\n",
    "  '''Remove non-informative words and characters (punctuation, extra space, ...)'''\n",
    "  tokens = text.split()\n",
    "  filtered_tokens = ' '.join([ i for i in tokens if not i.startswith('http') \n",
    "                                                and not i.startswith('@') \n",
    "                                                #and len(i) > 1 \n",
    "                            ])\n",
    "  return filtered_tokens\n",
    "\n",
    "train2 = train.copy()\n",
    "train2['tweet'] = train['tweet'].apply(cleanup)\n",
    "print('Accuracy: {}%'.format(run_model(train2['tweet'], train2['sentiment'], verbose=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features:  33821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60.37"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleanup(text):\n",
    "  '''Remove non-informative words and characters (punctuation, extra space, ...)'''\n",
    "  tokens = text.split()\n",
    "  filtered_tokens = ' '.join([ i for i in tokens if not i.startswith('http') \n",
    "                                                and not i.startswith('@') \n",
    "                                                and len(i) > 1 \n",
    "                            ])\n",
    "  return filtered_tokens\n",
    "\n",
    "train2 = train.copy()\n",
    "train2['tweet'] = train['tweet'].apply(cleanup)\n",
    "print('Accuracy: {}%'.format(run_model(train2['tweet'], train2['sentiment'], verbose=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381327"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = get_tokens(non_test['tweet'])\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 7235), ('RT', 7168), ('to', 7080), ('you', 5355), ('and', 3950), ('in', 3375), ('is', 3260), ('of', 3252), ('my', 3073), ('for', 3051), ('me', 2299), ('on', 2264), ('it', 2067), ('that', 2000), (\"I'm\", 1881), ('be', 1833), ('so', 1785), ('this', 1618), ('with', 1584), ('your', 1509), ('have', 1488), ('like', 1448), ('at', 1409), ('just', 1267), ('are', 1261), ('but', 1229), ('not', 1202), ('love', 1157), (\"don't\", 1067), ('was', 1024), ('all', 984), ('get', 973), ('up', 907), ('do', 906), ('can', 898), ('when', 874), ('&amp;', 844), ('out', 838), ('from', 818), ('if', 818), ('know', 783), ('we', 779), ('about', 778), ('will', 760), ('what', 747), ('The', 742), (\"it's\", 716), ('as', 711), ('they', 708), ('people', 687)]\n"
     ]
    }
   ],
   "source": [
    "a = [ i for i in Counter(tokens).most_common(50) if i not in custom_stopwords ]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features:  31092\n",
      "sklearn stopwords 58.17\n",
      "Number of Features:  31124\n",
      "sklearn stopwords cleaned 58.5\n",
      "Number of Features:  31246\n",
      "nltk stopwords 57.85\n",
      "Number of Features:  31276\n",
      "nltk stopwords cleaned 58.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vvaezian\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['amp', 're', 'rt', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features:  31304\n",
      "custome stopwords 58.47\n"
     ]
    }
   ],
   "source": [
    "# sklearn stopwords (default)\n",
    "print('sklearn stopwords', run_model(non_test['tweet'], non_test['sentiment'], vectorizer_params={'stop_words':'english'}, verbose=True))\n",
    "\n",
    "# sklearn stopwords (default) cleaned\n",
    "from sklearn.feature_extraction import _stop_words as stop_words\n",
    "scikit_stopwords = stop_words.ENGLISH_STOP_WORDS\n",
    "scikit_stopwords_cleaned = set(scikit_stopwords)\n",
    "# removing the following words from the list of stopwords as they could be helpful in determining sentiment\n",
    "for member in ['empty', 'less', 'too', 'alone', 'never', 'enough', 'can', 'everything', 'give', 'serious', \n",
    "               'will', 'always', 'couldnt', 'nobody', 'must', 'sincere', 'cant', 'down', 'cannot', 'cry', \n",
    "               'full', 'neither', 'nowhere', 'anything', 'nor', 'nothing', 'not', 'please', \n",
    "               'last', 'behind', 'out', 'every']:\n",
    "  scikit_stopwords_cleaned.remove(member)\n",
    "print('sklearn stopwords cleaned', run_model(non_test['tweet'], non_test['sentiment'], vectorizer_params={'stop_words':scikit_stopwords_cleaned}, verbose=True))\n",
    "\n",
    "# nltk stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = stopwords.words('english')\n",
    "print('nltk stopwords', run_model(non_test['tweet'], non_test['sentiment'], vectorizer_params={'stop_words':nltk_stopwords}, verbose=True))\n",
    "\n",
    "# nltk stopwords cleaned\n",
    "nltk_stopwords_cleaned=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"to\", \"from\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"more\", \"most\", \"other\", \"some\", \"such\", \"nor\", \"only\", \"own\", \"same\", \"so\", \"than\", \"very\", \"s\", \"t\", \"just\", \"don\", \"should\", \"now\"]\n",
    "print('nltk stopwords cleaned', run_model(non_test['tweet'], non_test['sentiment'], vectorizer_params={'stop_words':nltk_stopwords_cleaned}, verbose=True))\n",
    "\n",
    "# the following is taken from the 100 most common tokens in the data that seems to have no sentiment\n",
    "custom_stopwords = {'I', 'the', 'RT', 'to', 'a', 'you', 'and', 'in', 'is', 'of', 'my', 'for', 'me', 'on', 'it'\n",
    "  , 'that', \"I'm\", 'be', 'so', 'this', 'with', 'your', 'have', 'at', 'just', 'are', 'but', 'i' \n",
    "  , 'was', 'all', 'get', 'up', 'do', 'when', '&amp;', '-', 'from', 'if', 'know', 'we', 'about', 'what'\n",
    "  , 'The', \"it's\", 'as', 'they', 'u', 'people', 'one', 'by', 'no', 'see', 'go', 'You', 'how'\n",
    "  , 'or', 'an', 'got', 'who', 'he', 'more', 'day', 'make', 'time'\n",
    "  , \"you're\", 'My', 'A', 'has', 'really', 'much', 'now', 'some', \"It's\", 'back', 'If'\n",
    "  , 'would', 'still', 'going', 'his', 'been', 'new', 'only', 'her', 'even'\n",
    "  , \"I've\", 'them', 'than', 'our', \"that's\", 'This', 'had', '.', '2', 'there', 'she'\n",
    "  , 'then', 'last', 'over', 'life', 'say', 'come'\n",
    "  , 'their', 'am', 'because',\n",
    "}\n",
    "\n",
    "print('custome stopwords', run_model(non_test['tweet'], non_test['sentiment'], vectorizer_params={'stop_words':custom_stopwords}, verbose=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studies study studi\n",
      "studied studied studi\n",
      "studying studying studi\n",
      "loves love love\n",
      "loved loved love\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "for word in ['studies', 'studied', 'studying', 'loves', 'loved']:\n",
    "  print(word, lemmatizer.lemmatize(word), stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_animate_emoji = convert_animated_emojis(data)\n",
    "data_no_emoji = convert_text_emoji(data_no_animate_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver = 'liblinear', max_iter= 1000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.633"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model(data_no_emoji, data_labels, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "compresed_data = compress(data_no_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.634"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_model(compresed_data, data_labels, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ðŸ˜‚'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_emoji('xf0x9fx98x82')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize and find best accuracy for multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = range(1000, 4001, 200)\n",
    "Y = []\n",
    "best_acc = 0\n",
    "for n in X:\n",
    "  data = list(set(most_common(pos_lem_stem + neg_lem_stem, n)))\n",
    "  acc = run_model(data)\n",
    "  if acc > best_acc:\n",
    "    best_acc = acc\n",
    "  Y.append(acc) \n",
    "\n",
    "print('Best Accuracy: ', best_acc)\n",
    "plt.plot(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematization and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "  def __init__(self):\n",
    "    self.wnl = WordNetLemmatizer()\n",
    "  def __call__(self, articles):\n",
    "    #a = [self.wnl.lemmatize(compress(t)) for t in word_tokenize(articles) if '/' not in t and len(t) >= 2 and t.isalpha()]\n",
    "    #a = [self.wnl.lemmatize(compress(t), map_pos(nltk.pos_tag(t)[0][1])) for t in word_tokenize(articles) if '/' not in t and t.isalpha()]\n",
    "    a = [self.wnl.lemmatize(t, map_pos(nltk.pos_tag(t)[0][1])) for t in word_tokenize(articles) if '/' not in t and t.isalpha()]\n",
    "    return [i for i in a if len(i) > 1]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posStr = pos.lower() \n",
    "negStr = neg.lower()\n",
    "\n",
    "posStr2 = posStr.replace(\"\\'\", '')\n",
    "negStr2 = negStr.replace(\"\\'\", '')\n",
    "\n",
    "posStr = pos.lower().replace(\"n\\'t\", ' not')\n",
    "negStr = neg.lower().replace(\"n\\'t\", ' not')\n",
    "\n",
    "pos_cleaned = re.findall(\"[a-z][a-z]+\", posStr2)\n",
    "neg_cleaned = re.findall(\"[a-z][a-z]+\", negStr2)\n",
    "\n",
    "pos_lem_stem = [ lemmatizer.lemmatize(i) for i in pos_cleaned ]\n",
    "neg_lem_stem = [ lemmatizer.lemmatize(i) for i in neg_cleaned ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Misclassified Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_samples_pos = X_val[(y_val != y_pred) & (y_pred == 'pos')]\n",
    "misclassified_samples_neg = X_val[(y_val != y_pred) & (y_pred == 'neg')]\n",
    "res = vectorizer.inverse_transform(misclassified_samples_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.635\n"
     ]
    }
   ],
   "source": [
    "#model = LogisticRegression(**model_params, solver = 'liblinear', max_iter= 1000, random_state=0)\n",
    "model = LogisticRegression(solver = 'liblinear', max_iter= 1000, random_state=0)\n",
    "\n",
    "# for i in range(1000, 10001, 500):\n",
    "#   vocab = [i[0] for i in Counter(tokens).most_common(i)]\n",
    "#   run_model(data2_string_per_line, {'vocabulary':vocab})  # best 73.1\n",
    "\n",
    "# black_list = ['xe2x80xa6','xe2x80x9c', 'xe2x80x9d', 'xefxb8x8f', 'xe2x80x99']\n",
    "\n",
    "# for i in range(1000, 7501, 100):\n",
    "#   vocab = [i[0] for i in Counter(tokens).most_common(i)]\n",
    "#   print(i, run_model(data2_string_per_line, {'vocabulary':vocab}, model=model))  # best 73.1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
